<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>爬虫 - Tag - My New Hugo Site</title>
        <link>http://example.org/tags/%E7%88%AC%E8%99%AB/</link>
        <description>爬虫 - Tag - My New Hugo Site</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sat, 23 Jul 2022 17:58:00 &#43;0800</lastBuildDate><atom:link href="http://example.org/tags/%E7%88%AC%E8%99%AB/" rel="self" type="application/rss+xml" /><item>
    <title>Julia 简单爬虫编写</title>
    <link>http://example.org/posts/2022/07/julia-spider/</link>
    <pubDate>Sat, 23 Jul 2022 17:58:00 &#43;0800</pubDate>
    <author>Author</author>
    <guid>http://example.org/posts/2022/07/julia-spider/</guid>
    <description><![CDATA[项目介绍 我想试试 do everything in Julia，所以这里我简单写了一个爬虫，来爬取涩图 https://github.com/nesteiner/WebCrawl.jl 安装包 以下的包先安装好 HTTP 用于获取 HTTP Gumbo 用于解析 HTTP Cascadia 使用 CSS Selector 解析 HTTP URIs 用于uri 的解析 我们要爬取的 url 在这里 未成年人请在父母陪同下点击 工具函数 Julia 原有的 joinpath 不能满足我们的需求，比如我们要合并这两个 url https://example.com/hello /hello/1 合并后的结果应该是 https://example.com/hello/1 ，可 joinpath 合并后的结果是 /hello/1 这里我们使用 URIs 中的 resolvereference 来简单写一个 urljoin function urljoin(base::AbstractString, ref::AbstractString) return string( resolvereference(base, ref) ) end 整体流程 首先我们定义一个全局的 CONFIG 对象，来设置一些参数，比如 代理地址 请求头 Cookie User-Agent 我们再定义解析函数 parse ，由于处理方式与 scrapy 不太一样，没有必要将接口设计成一样 parse(startpage::String, dict::Dict{String, T}) where T &lt;: Any 由于出现分页，我们需要开始递归解析页面，我们从 startpage 开始解析， dict 存储一些额外参数，如 存储的文件夹位置 图片的名称 调用解析函数时，我们还会处理图片下载请求 pipeline(image::String, path::String) 他将从 image 下载资源，存储到 path 中 详细代码 参考 test/runtests.]]></description>
</item>
<item>
    <title>Scrapy 爬虫的简单使用</title>
    <link>http://example.org/posts/2022/07/scrapy-spider/</link>
    <pubDate>Sat, 02 Jul 2022 14:31:00 &#43;0800</pubDate>
    <author>Author</author>
    <guid>http://example.org/posts/2022/07/scrapy-spider/</guid>
    <description><![CDATA[简单的爬虫流程 一般的，程序向网站请求网页 程序 =========&gt; 网站 而后，网站返回网页与程序 程序 &lt;========= 网站 接着程序负责解析得到的 HTML 数据即可 Scrapy 如何扩展这一流程 程序请求网页时， scrapy 添加了中间件在双方之间 程序 =====&gt; 中间件 ======&gt; 网站 网站返回数据时，也需要通过中间件 程序 &lt;===== 中间件 &lt;====== 网站 程序解析完数据后，不会马上处理数据，而是把一个网页的数据打包成一个 Item 数据，传递给 pipeline 处理 pipeline &lt;==== item &lt;==== 程序 Scrapy 简单示例 这里以爬取美图录的图片为例(注意，我不打算用默认的 start_urls 数据) 首先我们到一个模特的主页上，比如 接下来我们要从这个 指定模特页面 上的每一个专辑上爬取图片 准备项目 安装 scrapy sudo pip3 install scrapy 创建新项目 scrapy startproject meitulu 基础设定 在 meitulu/spiders/ 下我们新建一个爬虫文件 evelyn.py class Spider(scrapy.Spider): name = &#39;evelyn&#39; 其中的 name 属性是为了调用的时候能找到爬虫位置，比如调用这个爬虫的时候，我们指定 name scrapy crawl evelyn 添加命令行参数 def __init__(self, albumUrl=None, *args, **kwargs): if albumUrl == None: raise Exception(&#39;usage: -a albumUrl=.]]></description>
</item>
</channel>
</rss>
